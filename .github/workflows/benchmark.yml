name: Performance tracking
on:
  pull_request:

env:
  PYTHON: ~

jobs:
  performance-tracking:
    runs-on: ubuntu-latest
    #runs-on: self-hosted
    steps:
      # setup
      - uses: actions/checkout@v4
      - uses: julia-actions/setup-julia@latest
        with:
          version: '1'
      - uses: julia-actions/julia-buildpkg@latest
      - name: install dependencies
        run: julia -e 'using Pkg; pkg"add PkgBenchmark BenchmarkCI@0.1"'

      # run the benchmark suite
      - name: run benchmarks (skip baseline fetch for forks)
        run: |
          julia -e '
            using BenchmarkCI
            success = true
            try
              BenchmarkCI.judge()
            catch err
              @info "BenchmarkCI.judge failed (likely missing baseline)" err=err
              success = false
            end
            if success
              try
                BenchmarkCI.displayjudgement()
              catch err
                @info "BenchmarkCI.displayjudgement failed (no results)" err=err
                success = false
              end
            end
            if !success
              mkpath(".benchmarkci")
              open(".benchmarkci/result-target.json", "w") do io
                write(io, "{}")
              end
              open("benchmark-result.artifact", "w") do io
                write(io, "Benchmark skipped on fork: missing baseline")
              end
            end
            '

      # generate and record the benchmark result as markdown
      - name: generate benchmark result
        run: |
          body=$(julia -e '
          try
              using BenchmarkCI
              let
                  judgement = BenchmarkCI._loadjudge(BenchmarkCI.DEFAULT_WORKSPACE)
                  title = "Benchmark Result"
                  ciresult = BenchmarkCI.CIResult(; judgement, title)
                  BenchmarkCI.printcommentmd(stdout::IO, ciresult)
              end
          catch err
              println("Benchmark skipped: $(err)")
          end
          ')
          body="${body//'%'/'%25'}"
          body="${body//$'\n'/'%0A'}"
          body="${body//$'\r'/'%0D'}"
          echo $body > ./benchmark-result.artifact

      # record the pull request number
      - name: record pull request number
        run: echo ${{ github.event.pull_request.number }} > ./pull-request-number.artifact

      # save as artifacts (performance tracking (comment) workflow will use it)
      - uses: actions/upload-artifact@v4
        with:
          name: performance-tracking
          path: ./*.artifact
